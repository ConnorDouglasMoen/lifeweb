{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP in Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The dataset \n",
    "2. Text processing with spaCy\n",
    "3. Automatic phrase modeling\n",
    "4. Topic modeling with LDA\n",
    "5. Visualizing topic models with pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dataset\n",
    "\n",
    "https://www.kaggle.com/residentmario/exploring-tripadvisor-uk-restaurant-reviews/data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "\n",
    "\n",
    "spaCy is an industrial-strength natural language processing (NLP) library for Python. spaCy's goal is to take recent advancements in natural language processing out of research papers and put them in the hands of users to build production software.\n",
    "\n",
    "spaCy handles many tasks commonly associated with building an end-to-end natural language processing pipeline:\n",
    "\n",
    "Tokenization <br/>\n",
    "Text normalization, such as lowercasing, stemming/lemmatization<br/>\n",
    "Part-of-speech tagging<br/>\n",
    "Syntactic dependency parsing<br/>\n",
    "Sentence boundary detection<br/>\n",
    "Named entity recognition and annotation<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'C:\\\\Python36\\\\python.exe', 'C:\\\\Users\\\\chenjf\\\\Desktop\\\\shell.w32-ix86', 'C:\\\\Users\\\\chenjf\\\\Anaconda3\\\\python36.zip', 'C:\\\\Users\\\\chenjf\\\\Anaconda3\\\\DLLs', 'C:\\\\Users\\\\chenjf\\\\Anaconda3\\\\lib', 'C:\\\\Users\\\\chenjf\\\\Anaconda3', 'C:\\\\Users\\\\chenjf\\\\Anaconda3\\\\lib\\\\site-packages', 'C:\\\\Users\\\\chenjf\\\\Anaconda3\\\\lib\\\\site-packages\\\\Babel-2.5.0-py3.6.egg', 'C:\\\\Users\\\\chenjf\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\chenjf\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\chenjf\\\\Anaconda3\\\\lib\\\\site-packages\\\\Pythonwin', 'C:\\\\Users\\\\chenjf\\\\Anaconda3\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\chenjf\\\\.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-54b68a8dc69e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import itertools as it\n",
    "\n",
    "import os\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c96a0eab0d11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#default english model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_sm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "#default english model\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-10cc7330ee37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnlp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in data\n",
    "\n",
    "data = pd.read_csv('C:/Users/chenjf/Desktop/data_science-master/data_science-master/restaurant_reviews/restaurant_reviews.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#take review_text field\n",
    "\n",
    "fields = [\"review_text\"]\n",
    "data = pd.read_csv('C:/Users/chenjf/Desktop/data_science-master/data_science-master/restaurant_reviews/restaurant_reviews.csv', encoding='utf-8', na_values=['NA'], usecols = fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate 0-9\n",
    "\n",
    "print(data['review_text'][0:9].str.cat(sep=' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use your own path file\n",
    "\n",
    "reviews_path = 'C:/Users/chenjf/Desktop/data_science-master/data_science-master/restaurant_reviews/sample_reviews.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#0:500 to one string\n",
    "\n",
    "sample_reviews = data['review_text'][0:200].str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print string\n",
    "\n",
    "sample_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_file = open('C:/Users/chenjf/Desktop/data_science-master/data_science-master/restaurant_reviews/sample_reviews.txt', \"w\", encoding=\"utf-8\")\n",
    "text_file.write(sample_reviews)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hand these reviews to spaCy, and be prepared to wait..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parse and tag\n",
    "\n",
    "parsed_reviews = nlp(sample_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parsed_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks the same. What did this do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence detection\n",
    "\n",
    "for num, sentence in enumerate(parsed_reviews.sents):\n",
    "    print('Sentence {}:'.format(num + 1))\n",
    "    print(sentence)\n",
    "    print('')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# entity detection\n",
    "# https://spacy.io/usage/linguistic-features\n",
    "\n",
    "\n",
    "for num, entity in enumerate(parsed_reviews.ents):\n",
    "    print('Entity {}:'.format(num + 1), entity, '-', entity.label_)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of speech tagging\n",
    "\n",
    "token_text = [token.orth_ for token in parsed_reviews]\n",
    "token_pos = [token.pos_ for token in parsed_reviews]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_pos)),\n",
    "             columns=['token_text', 'part_of_speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization\n",
    "# lemmatization, shape analysis\n",
    "\n",
    "\n",
    "token_lemma = [token.lemma_ for token in parsed_reviews]\n",
    "token_shape = [token.shape_ for token in parsed_reviews]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_lemma, token_shape)),\n",
    "             columns=['token_text', 'token_lemma', 'token_shape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about other token-level attributes?\n",
    "* relative frequency of tokens <br> \n",
    "* whether or not a token matches any of these categories: stopword, punctuation, whitespace, represents a number, whether or not the token is included in spaCy's default vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# token attributes\n",
    "\n",
    "token_attributes = [(token.orth_,\n",
    "                     token.is_stop,\n",
    "                     token.is_punct,\n",
    "                     token.is_space,\n",
    "                     token.like_num,\n",
    "                     token.is_oov)\n",
    "                    for token in parsed_reviews]\n",
    "\n",
    "df = pd.DataFrame(token_attributes,\n",
    "                  columns=['text',\n",
    "                           'stop?',\n",
    "                           'punctuation?',\n",
    "                           'whitespace?',\n",
    "                           'number?',\n",
    "                           'out of vocab.?'])\n",
    "\n",
    "df.loc[:, 'stop?':'out of vocab.?'] = (df.loc[:, 'stop?':'out of vocab.?']\n",
    "                                       .applymap(lambda x: u'Yes' if x else u''))\n",
    "                                               \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrase modeling is another approach to learning combinations of tokens that together represent meaningful multi-word concepts. We can develop phrase models by looping over the the words in our reviews and looking for words that co-occur (i.e., appear one after another) together much more frequently than you would expect them to by random chance. There's some fancy formula that our phrase models will use to determine whether two tokens $A$ and $B$ constitute a phrase. It involves a ratio of the number of times each token appears in the corpus and the number of times they appear in order, against the size of the corpus vocabulary. \n",
    "\n",
    "\n",
    "Once our phrase model has been trained, we can apply it to new text. When our model encounters two tokens in new text that identifies as a phrase, it will merge the two into a single new token.\n",
    "\n",
    "Phrase modeling is superficially similar to named entity detection in that you would expect named entities to become phrases in the model. But you would also expect multi-word expressions that represent common concepts, but aren't specifically named entities (such as happy hour) to also become phrases in the model.\n",
    "\n",
    "We turn to the indispensible gensim library to help us with phrase modeling — the Phrases class in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simultaneously perform phrase modeling with iterative data transformation:\n",
    "\n",
    "Segment text of complete reviews into sentences & normalize text <br/>\n",
    "First-order phrase modeling $\\rightarrow$ apply first-order phrase model to transform sentences<br/>\n",
    "Second-order phrase modeling $\\rightarrow$ apply second-order phrase model to transform sentences<br/>\n",
    "Apply text normalization and second-order phrase model to text of complete reviews<br/>\n",
    "We'll use this transformed data as the input for some higher-level modeling approaches in the following sections.\n",
    "\n",
    "First, let's define a few helper functions that we'll use for text normalization. In particular, the lemmatized_sentence_corpus generator function will use spaCy to:\n",
    "\n",
    "Iterate over the reviews <br/>\n",
    "Segment the reviews into individual sentences<br/>\n",
    "Remove punctuation and excess whitespace<br/>\n",
    "Lemmatize the text<br/>\n",
    "(and do so efficiently in parallel when data is huge, thanks to spaCy's nlp.pipe() function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    with codecs.open(filename) as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\n', ' ')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_reviews in nlp.pipe(line_review(filename), batch_size = 1000, n_threads=4):\n",
    "        for sent in parsed_reviews.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write this data back out to a new file (unigram_sentences_all), with one normalized sentence per line. We'll use this data for learning our phrase models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences_filepath = 'C:/Users/chenjf/Desktop/data_science-master/data_science-master/restaurant_reviews/unigram_sentences_all.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviews_path is path to sample_reviews.txt\n",
    "with codecs.open(unigram_sentences_filepath, 'w', encoding='utf-8') as f:\n",
    "    for sentence in lemmatized_sentence_corpus(reviews_path): \n",
    "        f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `unigram_sentences_all` file now is a large text file with one document/sentence per line —  Gensim's *LineSentence* class provides an iterator for working with other gensim components. It streams the documents/sentences from disk, so that you never have to hold the entire corpus in RAM at once. This allows you to scale your modeling pipeline up to potentially very large corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 0, 20):\n",
    "    print(u' '.join(unigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll learn a phrase model that will link individual words into two-word phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_model_filepath = 'C:/Users/chenjf/Desktop/data_science-master/data_science-master/restaurant_reviews/bigram_model_all.txt'\n",
    "\n",
    "bigram_model = Phrases(unigram_sentences)\n",
    "\n",
    "bigram_model.save(bigram_model_filepath)\n",
    "    \n",
    "# load the finished model\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained phrase model for word pairs, let's apply it to the review sentences data and explore the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences_filepath = 'C:/Users/chenjf/Desktop/data_science-master/data_science-master/restaurant_reviews/bigrammed_sentences_all.txt'\n",
    "\n",
    "\n",
    "with codecs.open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "    for unigram_sentence in unigram_sentences:\n",
    "        bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "        f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#look at a subset\n",
    "\n",
    "for bigram_sentence in it.islice(bigram_sentences, 20, 50):\n",
    "    print(u' '.join(bigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_reviews_filepath = 'C:/Users/chenjf/Desktop/data_science-master/data_science-master/restaurant_reviews/bigram_transformed_reviews_all.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list of stop words\n",
    "spacy.lang.en.English.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at this point, you would usually run your entire file through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with codecs.open(bigram_reviews_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "    for parsed_review in nlp.pipe(line_review(reviews_path)):\n",
    "            \n",
    "            # lemmatize the text, removing punctuation and whitespace\n",
    "            unigram_review = [token.lemma_ for token in parsed_review\n",
    "                              if not punct_space(token)]\n",
    "            \n",
    "            # apply the first-order phrase model\n",
    "            bigram_review = bigram_model[unigram_review]\n",
    "            \n",
    "            # remove any remaining stopwords\n",
    "            bigram_review = [term for term in bigram_review\n",
    "                              if term not in spacy.lang.en.English.Defaults.stop_words]\n",
    "            \n",
    "            # write the transformed review as a line in the new file\n",
    "            bigram_review = u' '.join(bigram_review)\n",
    "            f.write(bigram_review + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(u'Original:' + u'\\n')\n",
    "\n",
    "for review in it.islice(line_review(reviews_path), 0,1):\n",
    "    print(review)\n",
    "\n",
    "print(u'----' + u'\\n')\n",
    "print(u'Transformed:' + u'\\n')\n",
    "\n",
    "with codecs.open(bigram_reviews_filepath, encoding='utf_8') as f:\n",
    "    for review in it.islice(f, 0,1):\n",
    "        print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is fully unsupervised. The topics are \"discovered\" automatically from the data by trying to maximize the likelihood of observing the documents in your corpus, given the modeling assumptions. They are expected to capture some latent structure and organization within the documents, and often have a meaningful human interpretation for people familiar with the subject material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st step: learn the full vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_dictionary_filepath= 'C:/Users/chenjf/Desktop/data_science-master/data_science-master/restaurant_reviews/bigram_dict_all.dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_reviews = LineSentence(bigram_sentences_filepath)\n",
    "\n",
    "    # learn the dictionary by iterating over all of the reviews\n",
    "bigram_dictionary = Dictionary(bigram_reviews)\n",
    "    \n",
    "    # filter tokens that are very rare or too common from\n",
    "    # the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "bigram_dictionary.filter_extremes(no_below=5, no_above=0.2)\n",
    "bigram_dictionary.compactify()\n",
    "\n",
    "\n",
    "bigram_dictionary.save(bigram_dictionary_filepath)\n",
    "    \n",
    "# load the finished dictionary from disk\n",
    "bigram_dictionary = Dictionary.load(bigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Like many NLP techniques, LDA uses a simplifying assumption known as the bag-of-words model. In the bag-of-words model, a document is represented by the counts of distinct terms that occur within it. Additional information, such as word order, is discarded.\n",
    "\n",
    "Using the gensim Dictionary we learned to generate a bag-of-words representation for each review. The bigram_bow_generator function implements this. We'll save the resulting bag-of-words reviews as a matrix.\n",
    "\n",
    "\"bag-of-words\" abbreviated to bow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_bow_filepath = 'C:/Users/chenjf/Desktop/data_science-master/data_science-master/restaurant_reviews/bigram_bow_corpus_all.mm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigram_bow_generator(filepath):\n",
    "    \"\"\"\n",
    "    function to read reviews from a file\n",
    "    output: bag-of-words representation\n",
    "    \"\"\"\n",
    "    \n",
    "    for review in LineSentence(filepath):\n",
    "        yield bigram_dictionary.doc2bow(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate bag-of-words representations for all reviews and save them as a matrix\n",
    "MmCorpus.serialize(bigram_bow_filepath,\n",
    "                       bigram_bow_generator(bigram_sentences_filepath))\n",
    "    \n",
    "# load the finished bag-of-words corpus from disk\n",
    "bigram_bow_corpus = MmCorpus(bigram_bow_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the bag-of-words corpus, we're finally ready to learn our topic model from the reviews. We simply need to pass the bag-of-words matrix and Dictionary from our previous steps to LdaMulticore as inputs, along with the number of topics the model should learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model_filepath = 'C:/Users/chenjf/Desktop/data_science-master/data_science-master/restaurant_reviews/lda_model_all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LdaMulticore(bigram_bow_corpus,num_topics=10,\n",
    "                   id2word=bigram_dictionary, \n",
    "                   workers=2)\n",
    "    \n",
    "lda.save(lda_model_filepath)\n",
    "    \n",
    "# load the finished LDA model from disk\n",
    "lda = LdaMulticore.load(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each topic is represented as a mixture of tokens, you can manually inspect which tokens have been grouped together into which topics to try to understand the patterns the model has discovered in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topics(topic_number, topn=5):\n",
    "    print(u'{:10} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "\n",
    "    for term, frequency in lda.show_topic(topic_number, topn=25):\n",
    "        print(u'{:10} {:.3f}'.format(term, round(frequency, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics(topic_number = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyLDAvis includes a one-line function to take topic models created with gensim and prepare their data for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LDAvis_data_filepath = '/Users/victoriacabales/Documents/data_science/restaurant_reviews/ldavis_prepared.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#take topic models prepared by gensim and prepare data for visualization\n",
    "\n",
    "LDAvis_prepared = pyLDAvis.gensim.prepare(lda, bigram_bow_corpus,\n",
    "                                              bigram_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What an LDA visualization shows:\n",
    "1. Better interpretation of individual topics\n",
    "2. Relationships between different topics\n",
    "\n",
    "Distance: topics that are similar appear closer together, dissimilar topics appear farther apart <br/>\n",
    "Size: relative frequency of topic in dataset<br/>\n",
    "Bar chart: 30 most relevant terms<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec\n",
    "\n",
    "The goal of word vector embedding models, or word vector models for short, is to learn dense, numerical vector representations for each term in a corpus vocabulary. If the model is successful, the vectors it learns about each term should encode some information about the meaning or concept the term represents, and the relationship between it and other terms in the vocabulary. \n",
    "\n",
    "# I like ___ food.\n",
    "\n",
    "a) italian\n",
    "b) mexican\n",
    "c) pen\n",
    "d) chair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "bigram_sentences = LineSentence(bigram_sentences_filepath)\n",
    "word2vec_filepath = 'C:/Users/chenjf/Desktop/data_science-master/data_science-master/restaurant_reviews/word2vec_model_all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "food2vec = Word2Vec(bigram_sentences, size=100, window=5,\n",
    "                        min_count=20, sg=1, workers=4)\n",
    "\n",
    "food2vec.save(word2vec_filepath)\n",
    "\n",
    "\n",
    "        \n",
    "# load the finished model from disk\n",
    "food2vec = Word2Vec.load(word2vec_filepath)\n",
    "food2vec.init_sims()\n",
    "\n",
    "print(u'{} training epochs so far'.format(food2vec.train_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look up the topn most similar terms to token\n",
    "\n",
    "def get_related_terms(token, topn=10):\n",
    "    for word, similarity in food2vec.most_similar(positive=[token], topn=topn):\n",
    "        print(u'{:10} {}'.format(word, round(similarity, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_related_terms('restaurant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
