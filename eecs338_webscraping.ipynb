{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EECS 338 \n",
    "## Webscraping Notebook\n",
    "\n",
    "### 04/24/2019\n",
    "### This is a jupyter notebook to be used to scrape internet data from forevermissed.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import with_statement\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome('/Users/aea/Downloads/chromedriver')\n",
    "names = ['Smith', 'Johnson', 'Jones', 'Moen', 'Chen', 'Alwan', 'Briggs', 'Louis',\n",
    "         \"Williams\", \"Brown\", \"Garcia\", \"Rodriguez\", \"Miller\", \"Davis\"]\n",
    "# names = ['Moen', 'Smith']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "links = []  # create empty list for all links\n",
    "\n",
    "for name in names:\n",
    "    element_names = []    # create temporary list for element names\n",
    "    \n",
    "    # perform search by last name\n",
    "    browser.get('https://www.forevermissed.com/findmemorial/?q=' + name)\n",
    "    \n",
    "    # obtain search result line\n",
    "    search_result = browser.find_elements_by_class_name(\"bar\")\n",
    "    \n",
    "    # obtain number of people that match search result\n",
    "    for res in search_result:\n",
    "        # convert to string type\n",
    "        num_results = str(res.text)\n",
    "\n",
    "        # obtain the number of search results found\n",
    "        num_results = ''.join(filter(lambda x: x.isdigit(), num_results))\n",
    "        \n",
    "    # we have the number....\n",
    "    # num_results is the number of search results for the last name\n",
    "    num_results = int(num_results)    \n",
    "    offset = 0\n",
    "\n",
    "    # while loop that resets webpage to display 30 new individuals so that we can obtain 30 more links\n",
    "    while offset < num_results:\n",
    "        \n",
    "        # navigate to webpage with 30 search results\n",
    "        browser.get('https://www.forevermissed.com/findmemorial/?q=' + name + '&offset=' + str(offset))\n",
    "        \n",
    "        # increase the offset by 30 to obtain 30 new results next loop through\n",
    "        offset += 30\n",
    "    \n",
    "        # obtain the link for the individual\n",
    "        inputElement = browser.find_elements_by_class_name(\"memorial-link\")\n",
    "\n",
    "        # append the links to a list\n",
    "        for el in inputElement:\n",
    "            element_names.append(el.text)\n",
    "\n",
    "    # append the list of links to a larger list        \n",
    "    links.append(element_names)\n",
    "    \n",
    "# print(links)\n",
    "\n",
    "# create a new list of \"valid\" links (links that will not be too long)\n",
    "valid_links = []\n",
    "\n",
    "for i in range(0, len(links)):\n",
    "    for link in links[i]:\n",
    "        if len(str(link)) <= 47:\n",
    "            valid_links.append(link)\n",
    "            \n",
    "print(valid_links)\n",
    "\n",
    "# output links to a text file\n",
    "with open('memorial_links.txt', 'w') as f:\n",
    "    for _list in valid_links:\n",
    "        f.write(str(_list) + '\\n')          \n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### unable to retrieve only \"user_text\" text, so we will have to take out all \"txt user_text\" and \"body user_text\" text from the list...\n",
    "\n",
    "\n",
    "# (possible) next steps:    \n",
    "##   1) Implement 4 dictionaries:\n",
    "##     a) one will have the name of the person and the link\n",
    "##     b) one will have the name and tribute text of the person \n",
    "##     c) one will have the name and story text of the person\n",
    "##     d) one will have the name and age of person\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_links = open('memorial_links.txt', \"r\").readlines()\n",
    "\n",
    "valid_links = map(lambda s: s.strip(), valid_links)\n",
    "\n",
    "# for link in valid_links:\n",
    "#     link = link.strip('\\n')\n",
    "    \n",
    "    \n",
    "valid_links = list(valid_links)\n",
    "len(valid_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'james-albert-smith-jr.forevermissed.com'\n",
    "stuff = 'http://' + link + '/#stories'\n",
    "response = requests.get('http://' + link + '/#stories')\n",
    "# print(response.text) \n",
    "\n",
    "def has_six_characters(css_class):\n",
    "    return css_class is not None and len(css_class) == 9\n",
    "\n",
    "soup = bs(response.text, \"html.parser\")\n",
    "# print(soup.prettify())\n",
    "            \n",
    "# obtain the text from written story (class = \"user_text\")\n",
    "user_text = soup.find_all(class_= \"user_text\")\n",
    "\n",
    "print(stuff)\n",
    "print(user_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# page = ''\n",
    "# while page == '':\n",
    "#     try:\n",
    "#         page = requests.get(url)\n",
    "#         break\n",
    "#     except:\n",
    "#         time.sleep(5)\n",
    "#         continue\n",
    "\n",
    "# collect tribute text from the about tab (the home page)\n",
    "tribute_text = []\n",
    "v_links = valid_links[0:25]\n",
    "i = 0\n",
    "\n",
    "for link in valid_links:\n",
    "    i += 1\n",
    "    \n",
    "    response = ''\n",
    "    \n",
    "    while response == '':\n",
    "        try:   \n",
    "            # navigate to the home page of the respective url\n",
    "            response = requests.get('http://' + link)\n",
    "            # print(response.text)  \n",
    "\n",
    "            soup = bs(response.text, \"html.parser\")\n",
    "            # print(soup.prettify())\n",
    "\n",
    "            # obtain the text from written tribute (class = \"txt user_text\")\n",
    "            user_text = soup.findAll(attrs={'class':'txt user_text'})\n",
    "\n",
    "            # iterate through each instance and convert text to string type\n",
    "            for text in user_text:\n",
    "                tribute_text.append(str(text))\n",
    "                \n",
    "            print(i)\n",
    "            \n",
    "            break\n",
    "            \n",
    "        except:\n",
    "            # implement a 5 second pause\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "\n",
    "\n",
    "# collect text from the stories tab \n",
    "story_text = []\n",
    "\n",
    "# def is_user_text(css_class):\n",
    "#     return css_class is not None and len(css_class) == 9\n",
    "\n",
    "# for link in valid_links:\n",
    "# for link in v_links:\n",
    "\n",
    "#     response = ''\n",
    "    \n",
    "#     while response == '':  \n",
    "#         try:   \n",
    "#             # navigate to stories tab by adjusting the url\n",
    "#             response = requests.get('http://' + link + '/#stories')\n",
    "#             # print(response.text)  \n",
    "\n",
    "#             soup = bs(response.text, \"html.parser\")\n",
    "#             # print(soup.prettify())\n",
    "            \n",
    "#             # obtain the text from written story (class = \"user_text\")\n",
    "#             user_text = soup.find_all(\"div\", class_= \"user_text\")\n",
    "#             print(user_text)\n",
    "\n",
    "#             # iterate through each instance and convert text to string type and append to new list\n",
    "#             for text in user_text:\n",
    "#                 story_text.append(str(text))   \n",
    "\n",
    "#             break\n",
    "\n",
    "#         except:\n",
    "#             # implement a 5 second pause\n",
    "#             time.sleep(5)\n",
    "            \n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tribute_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(story_text)\n",
    "\n",
    "new_story_text = []\n",
    "\n",
    "\n",
    "var = story_text[12]\n",
    "print(var)\n",
    "\n",
    "for item in story_text:\n",
    "    print(item)\n",
    "    if 'body user_text' or 'txt user_text' in item:\n",
    "        next\n",
    "    else:\n",
    "        new_story_text.append(item)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_story_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This part of the script is used to clean the data sets from unnecessary characters\n",
    "\n",
    "cleaned_tribute_text = []\n",
    "cleaned_story_text = []\n",
    "\n",
    "# clean the tribute data\n",
    "for item in tribute_text:\n",
    "    \n",
    "    # strip the text of the unecessary elements\n",
    "    item = item.strip('\\n')\n",
    "    item = item.strip('<div class=\"txt user_text\">')\n",
    "    item = item.strip('</')\n",
    "    item = item.replace('\\n', ' ')\n",
    "    \n",
    "    cleaned_tribute_text.append(item)\n",
    "    \n",
    "print(cleaned_tribute_text[3000])\n",
    "\n",
    "\n",
    "# # clean the story data\n",
    "# for item in story_text:\n",
    "\n",
    "#     # strip the text of the unecessary elements\n",
    "#     item = item.strip('\\n')\n",
    "#     item = item.strip('<div class=\"txt user_text\">')\n",
    "#     item = item.strip('</')\n",
    "#     item = item.replace('\\n', ' ')\n",
    "    \n",
    "#     cleaned_story_text.append(item)\n",
    "    \n",
    "# print(cleaned_story_text)\n",
    "\n",
    "\n",
    "# output to different text files\n",
    "with open('cleaned_tribute_textV2.txt', 'w') as f:\n",
    "    for _list in cleaned_tribute_text:\n",
    "        f.write(str(_list) + '\\n')            \n",
    "f.close()\n",
    "\n",
    "\n",
    "# with open('cleaned_story_text.txt', 'w') as f:\n",
    "#     for _list in cleaned_story_text:\n",
    "#         f.write(str(_list) + '\\n')           \n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # do this for the \"Smith\" last names\n",
    "# memories_list = []\n",
    "\n",
    "# for link in links[3]:\n",
    "#     browser.get('http://' + link)\n",
    "    \n",
    "# #     WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.ID, \"page-tributes\")))\n",
    "# #     WebDriverWait(browser, 10).until(EC.presence_of_all_elements_located)\n",
    "    \n",
    "# #     WebDriverWait(browser, 2000)\n",
    "    \n",
    "# #     inputElement = browser.find_elements_by_css_selector(\"txt user_text\")\n",
    "#     inputElement = browser.find_elements_by_class_name(\".personal_phrase_font.long\")\n",
    "#     inputElement = browser.find_element(By.CSS_SELECTOR, \"txt user_text\")\n",
    "#     print(inputElement)\n",
    "    \n",
    "    \n",
    "#     for el in inputElement:\n",
    "#         print(el.text)\n",
    "# #         memories_list.append(el.text)\n",
    "\n",
    "    \n",
    "\n",
    "#     # navigate to the home page of the respective url\n",
    "#     response = requests.get('http://' + link)\n",
    "#     # print(response.text)  \n",
    "\n",
    "#     soup = bs(response.text, \"html.parser\")\n",
    "#     # print(soup.prettify())\n",
    "\n",
    "#     # obtain the text from written tribute (class = \"txt user_text\")\n",
    "#     user_text = soup.findAll(attrs={'class':'txt user_text'})\n",
    "\n",
    "#     # iterate through each instance and convert text to string type\n",
    "#     for text in user_text:\n",
    "#         tribute_text.append(str(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
